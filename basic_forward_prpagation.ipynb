{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPV6Fo6Z/xQkDx5HEGRlUUD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShrieVarshini2004/Deep-Learning/blob/main/basic_forward_prpagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def draw_neural_net():\n",
        "    print(\"\\nNeural Network Architecture:\")\n",
        "    print(\"\"\"\n",
        "       x₁       x₂\n",
        "        \\\\     /\n",
        "         \\\\   /\n",
        "       h₁(●)●(●)h₂\n",
        "          \\\\ /\n",
        "           ●\n",
        "           o\n",
        "    \"\"\")\n",
        "    print(\"Legend:\")\n",
        "    print(\"x₁, x₂: Input neurons\")\n",
        "    print(\"h₁, h₂: Hidden neurons\")\n",
        "    print(\"o: Output neuron\")\n",
        "    print(\"●: Connections with weights\")\n",
        "\n",
        "# Generate random weights and biases\n",
        "weights = np.around(np.random.uniform(size=6), decimals=2)  # 6 weights: 2 input to hidden, 2 hidden to output\n",
        "biases = np.around(np.random.uniform(size=3), decimals=2)    # 3 biases: 2 for hidden layer, 1 for output\n",
        "\n",
        "# Draw the network first\n",
        "draw_neural_net()\n",
        "\n",
        "# Print parameters with labels matching the diagram\n",
        "print(\"\\nNetwork Parameters:\")\n",
        "print(f\"Input to h1 weights: w1={weights[0]}, w2={weights[1]}, bias={biases[0]}\")\n",
        "print(f\"Input to h2 weights: w3={weights[2]}, w4={weights[3]}, bias={biases[1]}\")\n",
        "print(f\"Hidden to output weights: w5={weights[4]}, w6={weights[5]}, bias={biases[2]}\")\n",
        "\n",
        "# Input values\n",
        "x_1 = 0.5   # input 1\n",
        "x_2 = 0.85  # input 2\n",
        "print('\\nInput values: x₁ = {}, x₂ = {}'.format(x_1, x_2))\n",
        "\n",
        "# Calculate weighted sums for hidden layer\n",
        "# First hidden neuron\n",
        "z_11 = x_1 * weights[0] + x_2 * weights[1] + biases[0]\n",
        "# Second hidden neuron\n",
        "z_12 = x_1 * weights[2] + x_2 * weights[3] + biases[1]\n",
        "\n",
        "print('\\nHidden Layer Calculations:')\n",
        "print('z₁₁ = x₁*w₁ + x₂*w₂ + b₁ = {:.2f}*{:.2f} + {:.2f}*{:.2f} + {:.2f} = {:.4f}'.format(\n",
        "    x_1, weights[0], x_2, weights[1], biases[0], z_11))\n",
        "print('z₁₂ = x₁*w₃ + x₂*w₄ + b₂ = {:.2f}*{:.2f} + {:.2f}*{:.2f} + {:.2f} = {:.4f}'.format(\n",
        "    x_1, weights[2], x_2, weights[3], biases[1], z_12))\n",
        "\n",
        "# Apply activation function (sigmoid) to hidden layer\n",
        "a_11 = 1.0 / (1.0 + np.exp(-z_11))  # Activation of first hidden neuron\n",
        "a_12 = 1.0 / (1.0 + np.exp(-z_12))  # Activation of second hidden neuron\n",
        "\n",
        "print('\\nHidden Layer Activations (sigmoid):')\n",
        "print('a₁₁ = σ(z₁₁) = 1/(1 + e^-({:.4f})) = {:.4f}'.format(z_11, a_11))\n",
        "print('a₁₂ = σ(z₁₂) = 1/(1 + e^-({:.4f})) = {:.4f}'.format(z_12, a_12))\n",
        "\n",
        "# Calculate weighted sum for output layer\n",
        "z_2 = a_11 * weights[4] + a_12 * weights[5] + biases[2]\n",
        "\n",
        "print('\\nOutput Layer Calculation:')\n",
        "print('z₂ = a₁₁*w₅ + a₁₂*w₆ + b₃ = {:.4f}*{:.2f} + {:.4f}*{:.2f} + {:.2f} = {:.4f}'.format(\n",
        "    a_11, weights[4], a_12, weights[5], biases[2], z_2))\n",
        "\n",
        "# Apply activation function to output\n",
        "a_2 = 1.0 / (1.0 + np.exp(-z_2))\n",
        "\n",
        "print('\\nFinal Output:')\n",
        "print('o = σ(z₂) = 1/(1 + e^-({:.4f})) = {:.4f}'.format(z_2, a_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pam5OoFjoXI6",
        "outputId": "686942c4-d32c-42d2-d12d-bbe64878fe4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Neural Network Architecture:\n",
            "\n",
            "       x₁       x₂\n",
            "        \\     /\n",
            "         \\   /\n",
            "       h₁(●)●(●)h₂\n",
            "          \\ / \n",
            "           ● \n",
            "           o\n",
            "    \n",
            "Legend:\n",
            "x₁, x₂: Input neurons\n",
            "h₁, h₂: Hidden neurons\n",
            "o: Output neuron\n",
            "●: Connections with weights\n",
            "\n",
            "Network Parameters:\n",
            "Input to h1 weights: w1=0.72, w2=0.93, bias=0.05\n",
            "Input to h2 weights: w3=1.0, w4=0.37, bias=0.75\n",
            "Hidden to output weights: w5=0.06, w6=0.53, bias=0.25\n",
            "\n",
            "Input values: x₁ = 0.5, x₂ = 0.85\n",
            "\n",
            "Hidden Layer Calculations:\n",
            "z₁₁ = x₁*w₁ + x₂*w₂ + b₁ = 0.50*0.72 + 0.85*0.93 + 0.05 = 1.2005\n",
            "z₁₂ = x₁*w₃ + x₂*w₄ + b₂ = 0.50*1.00 + 0.85*0.37 + 0.75 = 1.5645\n",
            "\n",
            "Hidden Layer Activations (sigmoid):\n",
            "a₁₁ = σ(z₁₁) = 1/(1 + e^-(1.2005)) = 0.7686\n",
            "a₁₂ = σ(z₁₂) = 1/(1 + e^-(1.5645)) = 0.8270\n",
            "\n",
            "Output Layer Calculation:\n",
            "z₂ = a₁₁*w₅ + a₁₂*w₆ + b₃ = 0.7686*0.06 + 0.8270*0.53 + 0.25 = 0.7344\n",
            "\n",
            "Final Output:\n",
            "o = σ(z₂) = 1/(1 + e^-(0.7344)) = 0.6758\n"
          ]
        }
      ]
    }
  ]
}